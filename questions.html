
<!-- saved from url=(0033)https://patrickcollison.com/about -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <link rel="stylesheet" href="Kevin Sun/style.css">
    
    <title>Questions Â· Kevin Sun</title>
  </head>
  <body>
  <div id="menu">
  <span class="title">Kevin Sun</span>
  <ul>
    <li><a href="index.html">About</a></li>
    <li><a href="questions.html">Questions</a></li>
    <li><a href="answers.html">Answers</a></li>
  </ul>
  </div>
  <div id="left">&nbsp;</div>
  <div id="content">
  <div id="content">

  <p>I'm curious about way too many things in the world. Here are some of the most intruiging questions that I have! </p>
    
  <p><b>Can large language models (LLMs) reason?</b></p>
  <p>
    Large language models (LLMs) underpin state-of-the-art machine learning technologies like ChatGPT. 
    They absorb a large text database to develop a mathematical encoding of language called a generative pre-trained transformer (GPT).
    They are then are fine-tuned to produce different types of text (e.g. dialogue, novels, poetry). 
    When performing these tasks, these models will typically execute some variant of a "next word prediction" task---given some previous words, 
    they'll predict what the next sequence of words in the phrase should be.
    This rather simple approach to language understanding has generated some pretty <a href="https://openai.com/research/gpt-4">impressive results</a>.
  </p>
  <p>
    Somewhere around a year ago, I encountered the literature around these LLMs, and I've gone back and forth over whether they could possibly replicate human reasoning. 
    On one hand, I'm persuaded by the <a href="https://en.wikipedia.org/wiki/Language_game_(philosophy)">Wittgensteinian interpretation of language</a> that 
    possibly affirms LLMs having the capacity for reasoning. 
  
  <p>
    In short, a colloquial version of the argument goes something like this: whenever we are "reasoning", we are just developing various expressions of "reason" based on our language context. 
    Your stubborn relatives can always "win" any debate against you by just changing the definition of words; your math teacher can
    flunk the entire class by cleverly phrasing a word problem. "Reason" has no meaning alone---it's always bound by some linguistic context that
    mediates its expression. Now, if we can develop a schema that can capture this linguistic
    context (e.g. GPTs), we have functionally developed a model that captures reasoning.
  </p>

  <p>
    I thought this was game-set-match for the LLMs, but I've encountered a dearth of literature that contradicts this simple argument. 
    Noam Chompsky came out with this <a href="https://www.nytimes.com/2023/03/08/opinion/noam-chomsky-chatgpt-ai.html">opinion piece</a>,
    disputing the reduction of language to a complex statistical engine. Functionally, these neural networks purely attempt to calculate the
    "probability" of a sentence given only the words before it---a task that doesn't really seem to make too much sense on its own.
    "Biden passed the farm bill on October 22nd, 2023" isn't a more probable sequence of words than "Biden consorts with the aliens".
    They both obey grammar rules and the other conventions of language, so it's difficult to say that these models that calculate the probability
    of sentences are truly developing a meaningful representation of language itself. Instead, they might just be learning other information
    from the text database that make it seem like it truly understands language (and thus reason)---when in reality, it's just drawing extraneous
    correlations that suggest the President is more likely to pass a piece of legislation than collude with extraterrestrials. 
  </p>

  <p>
    There are some other objections that I've encountered as well. Erik Larson's book, <a href="https://www.hup.harvard.edu/catalog.php?isbn=9780674983519">The Myth of AI</a>
    speaks to the limitations of inductive systems like machine learning to simulate abductive reasoning. Perfect performance for a machine learning
    model would be no different than having an infinite dimensional regression---it's just a map of correlations from past data. It will not produce
    a theory that explains the multi-terabyte hydra of information through the language of causes and effects. There are some interesting arguments
    later in the book that point to whether this critique of machine learning can be applied to virtually any scientific field nowadays---most research
    papers in biomedicine seem to be regurgitations of convoluted statistics---so maybe it says more about the infiltration of myopic data science methodology
    in social science than some targeted critique of AI alone.
  </p>

  <p>
    I'm still at a loss for who's right. I used to be a complete AI skeptic, but GPT-4's radically changed my perspective on the matter.
    There's a good chance that we're just advanced statistical engines---in which case an LLM could simulate reasoning without any problems---
    but there's also a good chance that we're not---in which case an LLM will just continue to be an excellent auto-complete program, no more.
    It's safe to say that this will be on my mind for awhile.
  </p>
  
  </div>
  
  </div>
  </body></html>