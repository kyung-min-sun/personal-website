<!-- saved from url=(0033)https://patrickcollison.com/about -->
<html>
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <link rel="stylesheet" href="Kevin Sun/style.css" />

    <title>Questions Â· Kevin Sun</title>
  </head>
  <body>
    <div id="menu">
      <ul>
        <li><a href="index.html">About</a></li>
        <li><a href="questions.html">Questions</a></li>
        <li><a href="answers.html">Answers</a></li>
        <li><a href="thinking.html">Thoughts</a></li>
      </ul>
    </div>
    <div id="left">&nbsp;</div>
    <div id="content">
      <p>
        Some of my unstructured thoughts. Not really questions or answers. Not
        refined enough to be blog posts. Just a lot of my open-ended ruminations
        and rants into the void.
      </p>

      <p><b>Expanding the domain of human cognition</b></p>
      <p>
        One university professor once told me that the economy "never grows, it
        only finds ways to monetize previously unmonetizable things"; I think a
        similar principle applies to technology. We don't invent "new things",
        we merely subject a new domain to human cognition.
      </p>
      <p>
        Take the weather for example: before weather forecasts, we did not think
        about whether it'd be rainy later in the afternoon before walking
        outside -- but now that we have forecasts, this becomes to think about.
        We've progressed far beyond that; not only do we have climate models
        that are precise over extraordinarily long time horizons, there are
        nations that possess the ability to
        <a
          href="https://en.wikipedia.org/wiki/Cloud_seeding_in_the_United_Arab_Emirates"
          >engineer the weather</a
        >
        itself.
      </p>
      <p>
        Once you expand the domain of what is considered a human invention, then
        this re-definition of technology as the expansion of human cognition has
        more profound implications. If you take stories to be a human invention,
        then by our definition, stories are a technology that provides us a
        entry tunnel to our deeper values and morals.
      </p>
      <p>
        Explaining technology through this language may explain the oft-cited
        contemporary malaise of "cognitive overload". By definition, technology
        expands the domain of human cognition, it is unsurprising that humans
        are experiencing the overuse of cognitive force.
      </p>
      <p>
        Once we wielded the power to control the weather, the weather became our
        responsibility.
      </p>

      <p><b>Second-order theories of morality and epistemic humility</b></p>

      <p>
        I was captivated by an
        <a
          href="https://open.spotify.com/episode/5sYlXzYI4pHPiwWAqvEjSP?si=6b16cffe443d449d"
          >incredible podcast episode</a
        >
        between Alex O'Connor and David Wolpe last weekend. Wolpe describes what
        was to me a novel resolution to the problem of evil. In short, God must
        permit a universe with unnecessary suffering, else it is impossible for
        truly good humans to exist. If humans are only kind because there exists
        a certainty of negative consequence, then there are no naturally kind
        humans.
      </p>
      <p>
        What I find novel about this hypothetical universe is the
        <i>certainty</i> of negative consequence. It seems that perfect
        knowledge of the future is the only difference between our universe and
        this hypothetical one---if you can see infinitely into the future, then
        you have certainty of whether certain actions will bring about negative
        consequences for yourself.
      </p>
      <p>
        There is a grain of intuition that emerges from this emphasis on
        certainty: do fundamental moral values rely on the existence of
        imperfect information? If I was a selfish criminal and knew with
        absolute certainty that my neighbor would never find out about my
        theivery, my optimal strategy would be to steal from him every day. It
        is only because of my imperfect knowledge of my neighbor's intentions
        that I choose against it.
      </p>
      <p>
        One piece of fiction comes to mind while ruminating on this topic:
        <i>Dune</i>. A theme in the novel that I haven't heard discussed is the
        corrupting nature of prescience. One critical plot device is that as
        Paul Atreides acquires the power of omniscience, he becomes ever-more
        willing to wager the lives of billions to achieve his objective.
      </p>
      <p>
        There is something that feels intuitively wrong about the patterns of
        behavior that would likely emerge if people gambled on poker whilst
        having perfect knowledge of everyone else's cards. I would suspect that
        the sight of people betting their family's life savings over a hand of
        cards would be gut wrenching.
      </p>
      <p>
        Following that train of intuition, some part of me wonders whether our
        imperfect knowledge of the world is an evolutionary advantage. Would it
        truly produce a more stable society if we had perfect knowledge of each
        other and our actions? I could see our inability to neither hear each
        other's thoughts*** nor see into the future as somewhat of an
        evolutionary equilibrium point. Any closer to prescience, and we might
        be running against the collapse of civilization.
      </p>
      <p>
        ***Trisolorans from Three Body Problem come to mind. Maybe there's a
        science fiction angle to this.
      </p>

      <p><b>Consciousness, Abstraction, and Computers</b></p>
      <p>
        I always found it interesting how the term "abstraction" is thrown
        around without much thought amongst software engineers. Abstraction is a
        rather non-trivial philosophical concept; whether they exist at all
        outside of our conscious experience is still a very open question.
      </p>
      <p>
        I find it fascinating that abstractions "work". We can't quite describe
        what we're doing when we're generalizing an idea into its more abstract
        variant, but for some reason, this generalization appears to be
        necessary for us to develop any breakthrough in thinking. A question
        emerges: what would thinking look like sans any abstractions? Is it even
        possible to "think" without the orchestration of some limited set of
        abstract concepts?
      </p>
      <p>
        It seems extraordinarily important to develop insights into the human
        capacity for abstraction in light of recent advances in artificial
        intelligence. As it concerns the capacity for human thought, our
        assessment of machines seems to be bottlenecked by our understanding of
        ourselves.
      </p>
      <ul>
        <li>
          <a href="https://en.wikipedia.org/wiki/I_Am_a_Strange_Loop"
            >Expansion of recursion to the study of consciousness</a
          >
        </li>
        <li>
          <a href="https://en.wikipedia.org/wiki/Man_a_Machine">Monism</a>
        </li>
        <li>
          <a href="https://en.wikipedia.org/wiki/Christof_Koch"
            >Panpsychism meets computers</a
          >
        </li>
        <li>
          <a href="https://en.wikipedia.org/wiki/Artificial_consciousness"
            >Artificial consciousness</a
          >
        </li>
        <li>
          <a href="https://en.wikipedia.org/wiki/Holonomic_brain_theory"
            >Quantum brains</a
          >
        </li>
        <li>
          <a
            href="https://en.wikipedia.org/wiki/Neural_correlates_of_consciousness"
            >SOTA neural imaging for consiousness</a
          >
        </li>
      </ul>

      <p><b>Religion as a meta-cognitive structure</b></p>
      <p>
        I have a hypothesis that part of the mass secularization of society may
        be in part explained by the Flynn effect. We have an explosion of
        abstract reasoning skills in the general populace, causing the core
        function of religion to become continuously more obsolete.
      </p>
      <p>
        It appears to me that one of the primary functions of religion is its
        ability to develop a meta-cognitive framework for thought -- in essence,
        describing how one should think about the world. Any system of morality
        or value appears to envelop the world in some sort of interpretive
        framework; religion is no different. It operates from a fundamentally
        high level of abstraction; you area describing thoughts that have not
        even emerged yet, using a maximally generalizable grammar to explain all
        of life. Throughout history, large collectives of humans have adopted a
        comparatively small number of religions, orchestrating society under a
        unitary collective abstraction. It may be the case that as individual
        humans possess greater and greater abstract thinking skills, more humans
        can orchestrate individual abstractions without the need for some
        collective abstraction like religion.
      </p>

      <p><b>Rationalizing the irrational</b></p>
      <p>
        I'm puzzled by people who attempt to memorize the digits of pi by
        remembering some logical relationship between the numbers (e.g. telling
        yourself a story about what number comes next, describing algebraic
        relationships between digits, etc.). By virtue of pi being an irrational
        number, we know that these methods are not properly describing the
        nature of the number -- you're taking a fixed length random number and
        then attempting to rationalize each digit.
      </p>
      <p>This became a starting point for a few interesting thoughts:</p>
      <ul>
        <li>
          <p>
            <b
              >Are all the answers to our deepest problems contained in the
              number pi?</b
            >
          </p>
          <p>
            If the digits of pi serve as an infinite random number generator,
            then you pretty much have the proverbial infinite typewriting monkey
            contained within the number. You can develop some encoding schema
            between the digits of pi (ex: put it in base 26 and have it start
            outputting English characters), and then reap the benefits of all
            the secret knowledge contained in the digits.
          </p>
          <p>
            You can even index each digit of pi and then start counting units of
            time before you reach this final "secret" contained in pi. What's
            fascinating about this heuristic is that it appears to reframe
            solutions to our deepest scientific questions as a function of time
            and randomness. With enough randomness and time, you can solve any
            problem (classic Darwinian take).
          </p>
        </li>
        <li>
          <p>
            <b
              >What could this thought experiment say about the way we narrate
              the past?</b
            >
          </p>
          <p>
            We know that our world is composed of entities (ex: pi, e, circles)
            that escape the language of real numbers, so to narrate the causal
            forces that drive this world might be as fruitless as predicting the
            next digit of pi after memoizing the many digits beforehand. Perhaps
            when we're developing a history of the past (taking some fixed pivot
            point in time and then developing a narrative that explains the
            digits before it), we're narrating the time like someone trying to
            remember the digits of pi.
          </p>
          <p>
            After much authorship, you may very well have created a story with
            internal logical consistency and complete historical information,
            but the story itself cannot tell you what will happen next; just
            like how a story used to remember the digits of pi won't be able to
            tell you the next digit.
          </p>
        </li>
      </ul>

      <p><b>Social Darwinism</b></p>
      <p>
        I recently watched the <i>Kingdom of the Planet of the Apes</i> movie,
        and one element of the plot that stuck out was the primates insisting on
        "evolving" their society through the aid of human technology. The movie
        seems to have this embedded critique of agential theories of evolution -
        that natural selection is a process propelled by conscious decisions
        rather than inarticulable environmental pressures. Along this path, the
        movie appears to also architect a critique of intellect as the primary
        bottleneck for civilizational advancement; the apes are clearly
        underdeveloped as ethical agents, but they have surpassed in cognitive
        ability the average human. There is more to be said along these two
        ideas as critiques of social darwinism.
      </p>

      <p><b>Willpower / Ego Depletion</b></p>
      <p>
        Ever since I was in high school, I thought that willpower was a finite
        commodity. You had to budget the quantity of difficult tasks that you
        were to complete throughout the day, otherwise you'd just run out of
        "will". At the time, I was compelled by
        <a
          href="https://faculty.washington.edu/jdb/345/345%20Articles/Baumeister%20et%20al.%20(1998).pdf"
          >a study</a
        >
        that had subjects eat tasy / distasteful food before a difficult task
        and measured the likelihood of them completing that task.
      </p>
      <p>
        Now, looking back, it seems like the minimal test case of having
        participants eat different foods before challenging tasks was too
        limited to generalize well to all domains involving "willpower".
        Additionally, there appear that this particular study has some
        <a
          href="http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0147770"
          >replicability problems</a
        >.
      </p>
      <p>
        It appears that willpower-oriented explanations of human behavior are
        just observations of a placebo effect -- the more likely someone thinks
        that willpower is limited,
        <a
          href="https://hbr.org/2016/11/have-we-been-thinking-about-willpower-the-wrong-way-for-30-years"
          >you're more likely to feel less willpower.</a
        >
        I'd be fascinated to see whether people would live different lives if
        this belief were reversed.
      </p>

      <p><b>Probability</b></p>
      <p>
        Probability is not
        <a href="https://www.arameb.com/blog/2020/11/22/probability">"real"</a>.
      </p>
      <p>I feel like there's a tie between this and machine learning.</p>

      <p><b>Social Media Musings</b></p>

      <p>
        I think a more targeted analysis of what social media platforms
        <i>are</i> would be necessary for us to articulate what we intuitively
        feel like is wrong about them.
      </p>

      <p>
        Some dimensions of these platforms that make them suboptimal sources of
        meaningful knowledge:
      </p>

      <ul>
        <li>
          <p><i>How we access the knowledge</i></p>
          <p>
            One interpretation of test-oriented learning is the
            <i>obstruction</i> of knowledge. You're not given the answers. This
            learning method bakes in habitual time investment as a condition to
            accessing information.
          </p>
          <p>
            The bet is that this restriction will force students to study a
            wider breadth of knowledge (as opposed to the limited number of
            answers on the test key) and nudges them into spending more time
            wrestling with the material. For foundational knowledge domains that
            generalize well, it intuitively feels like this methodology fiats
            students that are well-trained in abstract reasoning.
          </p>
          <p>
            It appears that social media doesn't quite pair well with that
            knowledge acquisition formula. There's a pretty wide diversity of
            posts optimized for short-term engagement. It's difficult to develop
            any good abstractions given the user experienced presented by these
            platforms.
          </p>
        </li>
        <li>
          <p><i>What knowledge is shown on these platforms</i></p>
          <p>
            A normal person's normal thought will not help you in your abnormal
            situation.
          </p>
          <p>
            Most people on these platforms won't offer you amazing advice on
            problems beyond a certain point of specificity. Particular problems
            require particular information with empiricial tests and intuition
            build up over long periods of time.
          </p>
          <p>
            I'm thinking about a concept that I like to call the "inversion of
            the knowledge-experience hierarchy".
          </p>
          <p>
            We like in an age where kids are primarily consuming content made by
            other kids. You can say the same about teenagers and young adults.
            One possible side effect of this is the hollowing out of knowledge
            bases. We're not building on the paper trail of our forefathers;
            we're digging holes and then filling them in with short-term
            experiences. I'm curious (if not slightly worried) at where this
            will bring us in a few decades.
          </p>
          <p>
            Now, you might ask: what made the past different in this regard?
            Surely peers were learning from other peers back before social media
            existed.
          </p>
          <p>
            I'll say that one compelling distinction is that previously
            information had to be accessed far more intentionally than today.
            Information was dispersed at a far lower throughput with true
            physical barriers to access (e.g. internet timeouts / newspapers
            still on paper) that restricted its availability enough to require
            conscious choice.
          </p>
          <p>
            I'd be interested if there's any literature on whether we're
            approaching maximal information throughput in humans. It seems like
            an upper limit must exist (if nothing more than just the limitations
            to eyesight, hearing, reading).
          </p>
          <p>
            That is not to say that there is a limit to knowledge, which is a
            measure of information interpretation. An infinite number of
            interpretations may very well exist.
          </p>
        </li>
      </ul>
      <p><i>So what can we do about it?</i></p>
      <p>
        Common objections to social media regulation that reduce to the
        inevitability of short-term human preference optimization are
        unpersuasive to me. Even though nicotine heavily spikes dopamine levels,
        we were able to phase out cigarettes with targeted public information
        campaigns (though it seems like vapes are making a comeback). I think we
        can begin to invest some thought into mirroring the most successful
        tactics in grassroots anti-drug movements for social media consumption
        (and we should obviously do away with the tactics that didn't work).
      </p>
    </div>
  </body>
</html>
